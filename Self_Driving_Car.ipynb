{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Self_Driving_Car_Rushi_Kanjaria_16062020.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "s_hcA2UCSfYy"
      ],
      "authorship_tag": "ABX9TyPBKJZ/xlbbR2f7yfjzJNjm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RushiKanjaria/Design-a-self-driving-car-solution-using-Reinforcement-Learning-using-Gym-Environment/blob/master/Self_Driving_Car.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIOzmjhqAxmP",
        "colab_type": "text"
      },
      "source": [
        "#Design a self-driving car solution using Reinforcement Learning using Gym Environment\n",
        "## 16th June 2020\n",
        "#Created By: \n",
        "###Rushi Kanjaria, Marwadi University\n",
        "###Kala Hemanth, Aditya Engineering College\n",
        "###K. Jhansi Naga Indusri, V R Siddhartha engineering college\n",
        "###Apoorva Jindal, Techno India NJR Institute of Technology\n",
        "###Na-gasuri Alekhya, R.V.R & J.C College Of Engineering\n",
        "\n",
        "## Mentored By: Dr. Hiren Takkar, Leadingindia.ai, Bennet University\n",
        "\n",
        "#This project is a part of internship offered by leadingindia.ai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Sjui72ctAvs",
        "colab_type": "text"
      },
      "source": [
        "#Taxi-v3\n",
        "```\n",
        "+---------+\n",
        "|R: | : :G|\n",
        "| : | : : |\n",
        "| : : : : |\n",
        "| | : | : |\n",
        "|Y| : |B: |\n",
        "+---------+\n",
        "```\n",
        "\n",
        "####Description: \n",
        "There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
        "\n",
        "####Observations:\n",
        "There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations.\n",
        "\n",
        "####Passenger locations:\n",
        "- 0: R(ed)\n",
        "- 1: G(reen)\n",
        "- 2: Y(ellow)\n",
        "- 3: B(lue)\n",
        "- 4: in taxi\n",
        "\n",
        "####Destinations:\n",
        "- 0: R(ed)\n",
        "- 1: G(reen)\n",
        "- 2: Y(ellow)\n",
        "- 3: B(lue)\n",
        "\n",
        "####Actions:\n",
        "There are 6 discrete deterministic actions:\n",
        "- 0: move south\n",
        "- 1: move north\n",
        "- 2: move east \n",
        "- 3: move west \n",
        "- 4: pickup passenger\n",
        "- 5: dropoff passenger\n",
        "\n",
        "####Rewards:\n",
        "There is a reward of -1 for each action and an additional reward of +20 for delivering the passenger. There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally.\n",
        "\n",
        "####Rendering:\n",
        "- blue: passenger\n",
        "- magenta: destination\n",
        "- yellow: empty taxi\n",
        "- green: full taxi\n",
        "- other letters (R, G, Y and B): locations for passengers and destinations\n",
        "\n",
        "\n",
        "                           \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGWXK6VKX0dx",
        "colab_type": "text"
      },
      "source": [
        "###NOTE:\n",
        "We have used google colab to impliment, train and test the self driving car. Google colab already contains the gym interface so you can skip step:0 and move on to step:1 if you are using google colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX7M5QB0Xelz",
        "colab_type": "text"
      },
      "source": [
        "#Step:0 Installing the gym interface\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dSHlI33Xv9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install cmake 'gym[atari]' scipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdv43MCoPRj7",
        "colab_type": "text"
      },
      "source": [
        "#Step:1 Import the Libraries\n",
        "First, We have to import some libraries, which will help us to create our agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GShlwZvVPAUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np          #Numpy is for our Qtable\n",
        "import gym                  #OpenAI gym is for our environment, i.e., our Taxi Environment\n",
        "import random               #Random is for generating random numbers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZTH8plLQpqy",
        "colab_type": "text"
      },
      "source": [
        "#Step:2 Create the Enviornment\n",
        " \n",
        "\n",
        "*   We will use OpenAI Gym library to create our environment.\n",
        "*   It is composed of many environments that we can use to train our agent.\n",
        "*   We will be creating the Taxi environment.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4rY2HVqQkEv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "8c16c4c0-8f69-4eaa-a559-ea611b81f791"
      },
      "source": [
        "env = gym.make(\"Taxi-v3\")\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4UTaClIYj4Y",
        "colab_type": "text"
      },
      "source": [
        "#Step:3 Initialize the Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll-OJ802YyUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_episodes = 50000          #Total episodes we will use to train our algorithm\n",
        "total_test_episodes = 100       #Number of episodes we will use to test our algorithm\n",
        "max_steps = 99                  #Maximum steps an agent can take during an episode\n",
        "\n",
        "learning_rate = 0.7             #Learning rate\n",
        "gamma = 0.6                     #Discount rate\n",
        "\n",
        "#Exploration Parameters\n",
        "epsilon = 1.0                   #Exploration rate\n",
        "max_epsilon = 1.0               #Exploration probability at start(Max value the epsilon can have)\n",
        "min_epsilon = 0.01              #Minimum exploration probability(min value the epsilon can have)\n",
        "decay_rate = 0.01               #Exponential decay rate for exploration probability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_hcA2UCSfYy",
        "colab_type": "text"
      },
      "source": [
        "#Solving the enviornment without Reinforcement Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_Qra2wUQU3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.reset()\n",
        "total_epochs = []\n",
        "total_penalties = []\n",
        "total_reward = []\n",
        "frames = []\n",
        "\n",
        "for episode in range(total_test_episodes):\n",
        "  state = env.reset()\n",
        "  step = 0\n",
        "  done = False\n",
        "  epochs, penalties, reward = 0,0,0\n",
        "\n",
        "  while not done:\n",
        "    #env.render()\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "\n",
        "    if reward == -10:\n",
        "      penalties += 1\n",
        "\n",
        "    epochs += 1\n",
        "\n",
        "    total_reward.append(reward)\n",
        "\n",
        "    frames.append({'episode': episode, 'frames': env.render(mode='ansi'), 'state': state, 'action': action, 'reward': reward})\n",
        "\n",
        "    state = new_state\n",
        "  total_penalties.append(penalties)\n",
        "  total_epochs.append(epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYvcqwdHR4TF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "586673a9-d632-42f7-efc3-5dcc88f06c50"
      },
      "source": [
        "print(f\"Results after {total_test_episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {sum(total_epochs)/ total_test_episodes}\")\n",
        "print(f\"Average reward per timesteps: {sum(total_reward) / sum(total_epochs)}\")\n",
        "print(f\"Average penalties per episode: {sum(total_penalties) / total_test_episodes}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 198.37\n",
            "Average reward per timesteps: -3.860563593285275\n",
            "Average penalties per episode: 63.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cDrjqxoS4wV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_ts_nrl = sum(total_epochs)/ total_test_episodes\n",
        "avg_reward_nrl = sum(total_reward) / sum(total_epochs)\n",
        "avg_penalties_nrl = sum(total_penalties) / total_test_episodes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DJvnctXSob2",
        "colab_type": "text"
      },
      "source": [
        "#Solving the environment with Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdBSZx2MR3rB",
        "colab_type": "text"
      },
      "source": [
        "#Step:4 Create the Q-Table\n",
        "*  To create our Qtable, we need to know how rows and columns we need, i.e., our states and actions. We have to calculate the action_size and state_size.\n",
        "*  OpenAI Gym has a way to do that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Lk3ftXTViG4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "4580ee8f-dfd9-436e-a449-f96c7353b5c9"
      },
      "source": [
        "action_size = env.action_space.n        #returns how much actions our environment has\n",
        "print(\"Action Size = \", action_size)\n",
        "\n",
        "state_size = env.observation_space.n    #returns how much sate our environment has\n",
        "print(\"State Size = \", state_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action Size =  6\n",
            "State Size =  500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SpqRuGdWBmR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "d03388ec-8556-4de8-ef1d-8061ae477577"
      },
      "source": [
        "qtable = np.zeros((state_size,action_size))     #Initializing our Qtable as a null matrix\n",
        "print(qtable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wha4gllHa2dD",
        "colab_type": "text"
      },
      "source": [
        "#Step:5 Implement the Q learning algorithm\n",
        " **Q(s,a) ← Q(s,a) + α * [r + γ * maxa'Q(s',a') - Q(s,a)]**\n",
        "\n",
        "*  α - Learning Rate\n",
        "*  γ - Discout Rate\n",
        "\n",
        "Breaking it down into steps, we get\n",
        "\n",
        "*   Start exploring actions: For each state, select any one among all possible actions for the current state (s).\n",
        "*   Travel to the next state (s') as a result of that action (a).\n",
        "*   For all possible actions from the state (s') select the one with the highest Q-value.\n",
        "*   Update Q-table values using the equation.\n",
        "*   Set the next state as the current state.\n",
        "*   If goal state is reached, then end and repeat the process.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wFHBqBNa8DX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for episode in range(total_episodes):\n",
        "  #reset the environment\n",
        "  state = env.reset()\n",
        "  step = 0\n",
        "  done = False\n",
        "\n",
        "  for step in range(max_steps):\n",
        "    #choose an action a in the current state\n",
        "    #we first randomize a number\n",
        "    exp_exp_tradeoff = random.uniform(0,1)\n",
        "\n",
        "    #If this number is greater than epsilon, then we are in situation of exploitation (taking the biggest Q value for this state)\n",
        "    if exp_exp_tradeoff > epsilon:\n",
        "      action = np.argmax(qtable[state,:])\n",
        "\n",
        "    #else we will do a random choice, i.e., exploration\n",
        "    else:\n",
        "      action = env.action_space.sample()\n",
        "\n",
        "    #Now we are taking the action(a) and moving to the state(s') and getting the reward(r)\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "\n",
        "    #We update our Qtable using the Qlearning equation\n",
        "    qtable[state,action] = qtable[state,action] + learning_rate * (reward + gamma * (np.max(qtable[new_state, :])) - qtable[state,action])\n",
        "\n",
        "    #Updating our state\n",
        "    state = new_state\n",
        "\n",
        "    #if done, then finish the episode\n",
        "    if done == True:\n",
        "      break\n",
        "    \n",
        "  #We reduce the epsilon (as we need less exploration)\n",
        "  epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*episode)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJhPmqk3jveN",
        "colab_type": "text"
      },
      "source": [
        "#Evaluate our agent\n",
        "\n",
        "*   After 50,000 episodes, The training is finished and the updated Qtable has been genrated using Qlearning\n",
        "*   This Qtable will help our agent to take right action at any state to maximize the reward\n",
        "*  Now the next action is always selected using the best Qvalue\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf1zqfl4lLxl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.reset()\n",
        "total_epochs = []\n",
        "total_penalties = []\n",
        "total_reward = []\n",
        "frames = []\n",
        "\n",
        "for episode in range(total_test_episodes):\n",
        "  state = env.reset()\n",
        "  step = 0\n",
        "  done = False\n",
        "  epochs, penalties, reward = 0,0,0\n",
        "\n",
        "  while not done:\n",
        "    #env.render()\n",
        "    action = np.argmax(qtable[state,:])\n",
        "\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "\n",
        "    if reward == -10:\n",
        "      penalties += 1\n",
        "\n",
        "    epochs += 1\n",
        "\n",
        "    total_reward.append(reward)\n",
        "\n",
        "    frames.append({'episode': episode, 'frames': env.render(mode='ansi'), 'state': state, 'action': action, 'reward': reward})\n",
        "\n",
        "    state = new_state\n",
        "  total_penalties.append(penalties)\n",
        "  total_epochs.append(epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AymNI4W9D9dT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "outputId": "5a1cbf74-ebc9-4d0e-d60c-ee2e85fc6eab"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        \n",
        "        if frame['episode'] == 10:\n",
        "          break\n",
        "\n",
        "        print(f\"Episode: {frame['episode']}\")\n",
        "        print(frame['frames'])\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.6)\n",
        "        \n",
        "        \n",
        "        \n",
        "print_frames(frames)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 0\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | :\u001b[43m \u001b[0m:G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "Timestep: 120\n",
            "State: 88\n",
            "Action: 3\n",
            "Reward: -1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-343077815b5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-343077815b5d>\u001b[0m in \u001b[0;36mprint_frames\u001b[0;34m(frames)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Action: {frame['action']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Reward: {frame['reward']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nHhJQnzqYXO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "35c8c8ce-1f62-4d35-80ef-8ddcd104994c"
      },
      "source": [
        "print(f\"Results after {total_test_episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {sum(total_epochs)/ total_test_episodes}\")\n",
        "print(f\"Average reward per timestep: {sum(total_reward) / sum(total_epochs)}\")\n",
        "print(f\"Average penalties per episode: {sum(total_penalties) / total_test_episodes}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 13.26\n",
            "Average reward per timestep: 0.583710407239819\n",
            "Average penalties per episode: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32pLfC7yT3f1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_ts_rl = sum(total_epochs)/ total_test_episodes\n",
        "avg_reward_rl = sum(total_reward) / sum(total_epochs)\n",
        "avg_penalties_rl = sum(total_penalties) / total_test_episodes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFozWi23VDvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QMWQVpSURmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = [('Average rewards per move', avg_reward_nrl, avg_reward_rl),('Average number of penalties per episode', avg_penalties_nrl, avg_penalties_rl),('Average number of timesteps per trip', avg_ts_nrl, avg_ts_rl)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpnT25-LUsLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Algorithms = pd.DataFrame(data = models, columns=['Measure', \"Random agent's performance\", \"Q-learning agent's performance\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uLmmoHrVHcQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "6d07d36f-bc4c-4105-8414-5fab9a041ee1"
      },
      "source": [
        "Algorithms"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Measure</th>\n",
              "      <th>Random agent's performance</th>\n",
              "      <th>Q-learning agent's performance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Average rewards per move</td>\n",
              "      <td>-3.860564</td>\n",
              "      <td>0.58371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Average number of penalties per episode</td>\n",
              "      <td>63.120000</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Average number of timesteps per trip</td>\n",
              "      <td>198.370000</td>\n",
              "      <td>13.26000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   Measure  ...  Q-learning agent's performance\n",
              "0                 Average rewards per move  ...                         0.58371\n",
              "1  Average number of penalties per episode  ...                         0.00000\n",
              "2     Average number of timesteps per trip  ...                        13.26000\n",
              "\n",
              "[3 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x22Hpr1AlNSH",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh-LkUvc8rzN",
        "colab_type": "text"
      },
      "source": [
        "#References\n",
        "\n",
        "\n",
        "*   https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
        "*   https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/Taxi-v2/Q%20Learning%20with%20OpenAI%20Taxi-v2%20video%20version.ipynb\n",
        "*  https://gym.openai.com/envs/Taxi-v3/\n",
        "*  https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py\n",
        "\n"
      ]
    }
  ]
}